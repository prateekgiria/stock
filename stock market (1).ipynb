{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from pandas import datetime\n",
    "from pandas import DataFrame\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "import quandl #loading quandl package for data gathering\n",
    "from pandas import Series #to create required Series to be added to certain dataframes\n",
    "from math import sqrt  #in order to compute RMS values of observed and predicted values\n",
    "from statsmodels.tsa.ar_model import AR #to import Auto-Regression model required for predicting stock data\n",
    "from sklearn.metrics import mean_squared_error #to find MSE of expected an predicted values\n",
    "from matplotlib import pyplot #for interactive plotting of required graphs\n",
    "pyplot.style.use('fivethirtyeight') #maintain uniform style of visualisation\n",
    "quandl.ApiConfig.api_key = \"cYf2RmzUcPBMGfsBrxhR\" #Quandl API key in order to obtain data from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gathering data from Quandl\n",
    "icici = quandl.get('NSE/ICICIBANK.5',start_date='2017-01-01',end_date='2017-12-26')\n",
    "bobr = quandl.get('NSE/BANKBARODA.5',start_date='2017-01-01',end_date='2017-12-26')\n",
    "boi = quandl.get('NSE/BANKINDIA.5',start_date='2017-01-01',end_date='2017-12-26' )\n",
    "canbk = quandl.get('NSE/CANBK.5',start_date='2017-01-01',end_date='2017-12-26' )\n",
    "fedbk = quandl.get('NSE/FEDERALBNK.5',start_date='2017-01-01',end_date='2017-12-26' )\n",
    "axis = quandl.get('NSE/AXISBANK.5',start_date='2017-01-01',end_date='2017-12-26' )\n",
    "hdfc = quandl.get('NSE/HDFCBANK.5',start_date='2017-01-01',end_date='2017-12-26' )\n",
    "indus = quandl.get('NSE/INDUSINDBK.5',start_date='2017-01-01',end_date='2017-12-26' )\n",
    "kotak = quandl.get('NSE/KOTAKBANK.5' ,start_date='2017-01-01',end_date='2017-12-26')\n",
    "pnb = quandl.get('NSE/PNB.5',start_date='2017-01-01',end_date='2017-12-26' )\n",
    "sbin = quandl.get('NSE/SBIN.5',start_date='2017-01-01',end_date='2017-12-26' )\n",
    "yesbk = quandl.get('NSE/YESBANK.5',start_date='2017-01-01',end_date='2017-12-26' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dictionary of all the stocks in a portfolio\n",
    "d1 = {'ICICI':icici['Close'], 'BOB':bobr['Close'],'BOI':boi['Close'], 'CANBK':canbk['Close'],'FEDBK':fedbk['Close'], \n",
    "     'AXIS':axis['Close'],'HDFC':hdfc['Close'],'INDUSIND':indus['Close'],'KOTAK':kotak['Close'], 'PNB':pnb['Close'],\n",
    "     'SBI':sbin['Close'], 'YESBANK':yesbk['Close']} \n",
    "\n",
    "df1 = pd.DataFrame(data=d1) #creating the dataframe\n",
    "df1 #printing the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1.to_csv(\"closing prices.csv\") #writing the dataframe to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys=[] #blank list to assign all stock names\n",
    "keys=df1.keys() #appending all stock names from dataframe to the list\n",
    "\n",
    "for i in range(len(df1.columns)): #loop to iterate over the stock names in the dataframe\n",
    "    df1[keys[i]].plot() #plotting the closing prices of each stock\n",
    "    \n",
    "pyplot.xlim([df1.index[0],df1.index[-1]]) #assigning index values(date) on x-axis\n",
    "pyplot.legend(loc=\"best\") #assigning the legend location to best suit the plot\n",
    "pyplot.show() #displaying the plot of closing prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#declaring function to predict values\n",
    "def predict(coef, history): \n",
    "    yhat = 0.0 #initialising variable to store predicted value\n",
    "    \n",
    "    for i in range(1, len(coef)+1): #loop to iterate over model co-efficient parameters\n",
    "        yhat += coef[i-1] * history[-i] #calculating predicted value\n",
    "    return yhat #returning Predicted variable to be computed in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#function to perform AR calculation\n",
    "def autoreg(ar):\n",
    "    global ar_mse #variable 'ar_mse' stores the Mean Squared Error between the Test Data and Predicted Values\n",
    "    global ard #variable to create a dataframe of test and predicted values\n",
    "    size = int(len(ar)/2) #Defining size of Train and Test Data\n",
    "    ar_train, ar_test = ar[0:size], ar[size:] #Assigning range of values for Train and Test Data\n",
    "    ar_history = [ar for ar in ar_train] #list comprehension for iterating through each element in train data\n",
    "    ar_predictions = list() # list to store Predicted Values\n",
    "    for t in range(len(ar_test)): #loop to iterate through the test data\n",
    "        ar_model = ARMA(ar_history, order=(1,0)) #Creating AR Model\n",
    "        ar_model_fit = ar_model.fit(trend='nc', disp=0) #obtaining the model values\n",
    "        ar_coef = ar_model_fit.arparams #computing AR parameters from the model\n",
    "        ar_yhat = predict(ar_coef, ar_history) #running function to generate predicted value\n",
    "        ar_predictions.append(ar_yhat) #appending predicted values to the predictions list\n",
    "        ar_obs = ar_test[t] #assigning observed value from test data to variable 'ar_obs'\n",
    "        ar_history.append(ar_obs) #appending test data to the train data\n",
    "        print('>predicted=%.3f, expected=%.3f' % (ar_yhat, ar_obs)) #printing the expected value and its corresponding test value\n",
    "    ar_mse=mean_squared_error(ar_test, ar_predictions) #calculating Mean Squared Error between Observed and Predicted Values\n",
    "    ard=pd.DataFrame({'Expected':ar_test,'Predicted':ar_predictions}) #creating a dataframe of observed and predicted values\n",
    "    print('Test Root Mean Squared Error: %.3f' % sqrt(ar_mse)) #Printing the computed Root Mean Squared Error\n",
    "    print(ar_model_fit.summary()) #Displaying the summary of AR Model Parameters\n",
    "    pyplot.plot(ar.index[len(ar_test)-1:],ar_test, color='black',label='test') #plotting the Observed Values\n",
    "    pyplot.plot(ar.index[len(ar_test)-1:],ar_predictions, color='red',label='predictions') #plotting the Predicted values\n",
    "    pyplot.legend(loc='best') #Assigning location of graph legend to be best suited\n",
    "    pyplot.title('Auto Rergression Model') #Title Of Graph\n",
    "    pyplot.show() #Displaying the AR Model plot\n",
    "    return autoreg #returning the entire function to be called by passing necessary Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MA MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to perform MA calculation\n",
    "def movavg(ma):\n",
    "    global ma_mse #variable 'ma_mse' stores the Mean Squared Error between the Test Data and Predicted Values\n",
    "    global mad #variable to create a dataframe of test and predicted values\n",
    "    size = int(len(ma)/2) #Defining size of Train and Test Data\n",
    "    ma_train, ma_test = ma[0:size], ma[size:] #Assigning range of values for Train and Test Data\n",
    "    ma_history = [ma for ma in ma_train] #list comprehension for iterating through each element in train data\n",
    "    ma_predictions = list() # list to store Predicted Values\n",
    "    for t in range(len(ma_test)): #loop to iterate through the test data\n",
    "        ma_model = ARMA(ma_history, order=(0,1)) #Creating AR Model\n",
    "        ma_model_fit = ma_model.fit(trend='nc', disp=False) #obtaining the model values\n",
    "        ma_coef = ma_model_fit.maparams #computing MA parameters from the model\n",
    "        ma_yhat = predict(ma_coef, ma_history) #running function to generate predicted value\n",
    "        ma_predictions.append(ma_yhat) #appending predicted values to the predictions list\n",
    "        ma_obs = ma_test[t] #assigning observed value from test data to variable 'ma_obs'\n",
    "        ma_history.append(ma_obs) #appending test data to the train data\n",
    "        print('>predicted=%.3f, expected=%.3f' % (ma_yhat, ma_obs)) #printing the expected alue and its corresponding test value\n",
    "    ma_mse = mean_squared_error(ma_test, ma_predictions) #calculating Mean Squared Error between Observed and Predicted Values\n",
    "    mad=pd.DataFrame({'Expected':ma_test,'Predicted':ma_predictions}) #creating a dataframe of observed and predicted values\n",
    "    print('Test Root Mean Squared Error: %.3f' % sqrt(ma_mse)) #Printing the computed Root Mean Squared Error\n",
    "    print(ma_model_fit.summary()) #Displaying the summary of MA Model Parameters\n",
    "    pyplot.plot(icici.index[len(ma_test)-1:],ma_test, color='black',label='test') #plotting the Observed Values\n",
    "    pyplot.plot(icici.index[len(ma_test)-1:],ma_predictions, color='red',label='predictions') #plotting the Predicted Values\n",
    "    pyplot.legend(loc='best') #Assigning location of graph legend to be best suited\n",
    "    pyplot.title('Moving Average Model') #Title Of Graph\n",
    "    pyplot.show() #Displaying the MA Model plot\n",
    "    return movavg #returning the entire function to be called by passing necessary Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARMA MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#declaring function to compute successive differences of 2 elements within dataset\n",
    "def difference(dataset):\n",
    "    diff = list() #creating list to store the difference of successive elements in test data\n",
    "    \n",
    "    for i in range(1, len(dataset)): #loop to iterate over each element in the dataset\n",
    "        value = dataset[i] - dataset[i - 1] #computing difference of succesive values\n",
    "        diff.append(value) #appending difference value to list\n",
    "    return np.array(diff) #returning the list of difference values as array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#function to perform ARMA calculation\n",
    "def armamod(arma):\n",
    "    global arma_mse,armad\n",
    "    arma_size = int(len(arma)/2) #Defining size of Train and Test Data\n",
    "    arma_train, arma_test = arma[0:arma_size], arma[arma_size:] #Assigning range of values for Train and Test Data\n",
    "    arma_history = [arma for arma in arma_train] #list comprehension for iterating through each element in train data\n",
    "    arma_predictions = list() # list to store Predicted Values\n",
    "    for t in range(len(arma_test)):\n",
    "        arma_model = ARMA(arma_history, order=(1,0)) #Creating ARMA Model\n",
    "        arma_model_fit = arma_model.fit(disp=0) #obtaining the model values\n",
    "        ar_coef = arma_model_fit.arparams #computing AR parameters from the model\n",
    "        ma_coef = arma_model_fit.maparams #computing MA parameters from the model\n",
    "        arma_resid = arma_model_fit.resid #obtaining residual parameters from ARMA model\n",
    "        arma_diff = difference(arma_history) #running the function to obtain difference between successive values\n",
    "        arma_yhat = predict(ar_coef, arma_history)+predict(ma_coef, arma_diff) #computing the predicted values\n",
    "        arma_predictions.append(arma_yhat) #appending predicted values to the predictions list\n",
    "        arma_obs = arma_test[t] #assigning observed value from test data to variable 'arma_obs'\n",
    "        arma_history.append(arma_obs) #appending test data to the train data\n",
    "        print('>predicted=%.3f, expected=%.3f' % (arma_yhat, arma_obs)) #printing the expected value and its corresponding test value\n",
    "    arma_mse = mean_squared_error(arma_test, arma_predictions)  #calculating Mean Squared Error between Observed and Predicted Values\n",
    "    armad=pd.DataFrame({'Expected':arma_test,'Predicted':arma_predictions}) #creating a dataframe of observed and predicted values\n",
    "    print('Test Root Mean Squared Error: %.3f' % sqrt(arma_mse)) #Printing the computed Root Mean Squared Error\n",
    "    print(arma_model_fit.summary()) #Displaying the summary of ARMA Model Parameters\n",
    "    pyplot.plot(icici.index[len(arma_test)-1:],arma_test, color='black',label='test') #plotting the Observed Values\n",
    "    pyplot.plot(icici.index[len(arma_test)-1:],arma_predictions, color='red',label='predictions') #plotting the Predicted Values\n",
    "    pyplot.legend(loc='best') #Assigning location of graph legend to be best suited\n",
    "    pyplot.title('ARMA Model') #Title Of Plot\n",
    "    pyplot.show() ##Displaying the ARMA Model plot with Observed and Predicted Values\n",
    "    return armamod #returning the entire function to be called by passing necessary Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate an ARIMA model for a given order (p,d,q)\n",
    "def evaluate_arima_model(X, arima_order):\n",
    "    train_size = int(len(X)/2) # prepare training dataset\n",
    "    train, test = X[0:train_size], X[train_size:] #dividing the data into train and test data\n",
    "    history = [x for x in train] #list comprehension for iterating through each element in train data\n",
    "    \n",
    "    predictions = list() # list to store Predicted Values\n",
    "    for t in range(len(test)): #loop to iterate through the test data\n",
    "        model = ARIMA(history, order=arima_order) #creating ARIMA model\n",
    "        model_fit = model.fit(disp=0) #obtaining the model values\n",
    "        yhat = model_fit.forecast()[0] #predicting the forecast for 1 step\n",
    "        predictions.append(yhat) #appending predicted values to the predictions list\n",
    "        history.append(test[t]) #appending test data to the train data\n",
    "        \n",
    "    error = np.sqrt(mean_squared_error(test, predictions)) # calculate out of sample error\n",
    "    return error #returning error to be passed on the subsequent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate combinations of p, d and q values for an ARIMA model\n",
    "def evaluate_models(dataset, p_values, d_values, q_values):\n",
    "    dataset = dataset.astype('float32')\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    for p in p_values: #loop to iterate through all values of AR order in ARIMA Model\n",
    "        for d in d_values: #loop to iterate through all values of difference degree in ARIMA Model\n",
    "            for q in q_values: #loop to iterate through all values of MA order in ARIMA Model\n",
    "                order = (p,d,q) #variable 'order' takes a tuple of the corresponding p,d,q order for the ARIMA model\n",
    "                try: #try block to assess the lowest MSE and best ARIMA order\n",
    "                    mse = evaluate_arima_model(dataset, order) #variable 'mse' stores the MSE obtained from the previous function\n",
    "                    if mse < best_score: #condition to check for lowest Mean Squared Error\n",
    "                        best_score, best_cfg = mse, order #variable 'best_score' stores lowest MSE and best_cfg stores the corresponding ARIMA order for the lowest MSE\n",
    "                        print('ARIMA%s MSE=%.3f' % (order,mse)) #to print the iterated ARIMA orders with their corresponding MSE\n",
    "                except:\n",
    "                    continue\n",
    "    return best_cfg #returns the ARIMA order for lowest MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#function to perform ARIMA calculation\n",
    "def arimamod(arima): \n",
    "    global arima_mse #variable 'arima_mse' stores the Mean Squared Error between the Test Data and Predicted Values\n",
    "    global arimad #variable to create a dataframe of test and predicted values\n",
    "    arima_size = int(len(arima)/2) #Defining size of Train and Test Data\n",
    "    arima_train, arima_test = arima[0:arima_size], arima[arima_size:] #Assigning range of values for Train and Test Data\n",
    "    arima_history = [arima for arima in arima_train] #list comprehension to assess each element in train data\n",
    "    arima_predictions = list() #declaring a list to store the predicted values after ARIMA predictions\n",
    "    order=evaluate_models(arima, p_values, d_values, q_values) #running function to obtain best ARIMA order\n",
    "    for t in range(len(arima_test)): #loop to iterate through the test data\n",
    "        arima_model = ARIMA(arima_history, order)#Creating Arima Model\n",
    "        arima_model_fit = arima_model.fit(disp=0) #Obtaining Model Parameters\n",
    "        ar_coef= arima_model_fit.arparams #computing AR parameters from the model\n",
    "        ma_coef = arima_model_fit.maparams #computing MA parameters from the model\n",
    "        arima_resid = arima_model_fit.resid #obtaining residual parameters from ARMA model\n",
    "        arima_diff = difference(arima_history) #running the function to obtain difference between successive values\n",
    "        arima_yhat = arima_history[-1]+ predict(ar_coef, arima_diff) + predict(ma_coef, arima_resid) #computing the predicted values\n",
    "        arima_predictions.append(arima_yhat)#appending predicted values to the predictions list\n",
    "        arima_obs = arima_test[t] #assigning observed value from test data to variable 'arma_obs'\n",
    "        arima_history.append(arima_obs) #appending test data to the train data\n",
    "        print('>predicted=%.3f, expected=%.3f' % (arima_yhat, arima_obs))\n",
    "    arima_mse = mean_squared_error(arima_test, arima_predictions)\n",
    "    arimad=pd.DataFrame({'Expected':arima_test,'Predicted':arima_predictions})\n",
    "    print('Test Root Mean Squared Error: %.3f' % sqrt(arima_mse)) #Printing the computed Root Mean Squared Error\n",
    "    print(arima_model_fit.summary()) #Displaying the summary of ARIMA Model Parameters\n",
    "    pyplot.plot(icici.index[len(arima_test)-1:],arima_test, color='black',label='test') #plotting the Observed Values\n",
    "    pyplot.plot(icici.index[len(arima_test)-1:],arima_predictions, color='red',label='predictions')  #plotting the Predicted Values\n",
    "    pyplot.legend(loc='best') #Assigning location of graph legend to be best suited\n",
    "    pyplot.title('ARIMA Model') #Title Of Plot\n",
    "    pyplot.show() #Displaying the ARIMA Model plot with Observed and Predicted Values\n",
    "    print(order) #Displaying the best order with lowest RMSE for the ARIMA model\n",
    "    return arimamod #returning the entire function to be called by passing necessary Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA models are denoted with the notation ARIMA(p, d, q). \n",
    "#Together these three parameters account for seasonality, trend, and noise in datasets:\n",
    "\n",
    "p_values = range(0, 4) #range to describe AR order of ARIMA Model\n",
    "d_values = range(0, 4) #range of degree of differencing\n",
    "q_values = range(0, 4) #range to describe MA order of ARIMA Model\n",
    "warnings.filterwarnings(\"ignore\") #to ignore warnings as it can give rise to errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm=[] #list to store RMSE values of all stocks obtained from AR Model\n",
    "mam=[] #list to store RMSE values of all stocks obtained from MA Model\n",
    "armam=[] #list to store RMSE values of all stocks obtained from ARMA Model\n",
    "arimam=[] #list to store RMSE values of all stocks obtained from ARIMA Model\n",
    "\n",
    "for i in range(len(df1.columns)): #loop to iterate through all stocks in the portfolio\n",
    "    \n",
    "    autoreg(df1[keys[i]]) #Running AR Model for all the stocks\n",
    "    arm.append('%.3f'%sqrt(ar_mse)) #appending all RMSE values of the different stocks from AR Model to list 'arm'\n",
    "    #Create CSV file for predicted values of the corresponding model using the name of the corresponding stock as part of filename\n",
    "    with open('AR of ' + str(df1.columns[i]) + '.csv', 'w') as csvfile:\n",
    "        ard.to_csv(csvfile) #writing the dataframe of observed and predicted values from AR Model to CSV file\n",
    "    \n",
    "    movavg(df1[keys[i]]) #Running MA Model for all the stocks\n",
    "    mam.append('%.3f'%sqrt(ma_mse)) #appending all RMSE values of the different stocks from MA Model to list 'mam'\n",
    "    #Create CSV file for predicted values of the corresponding model using the name of the corresponding stock as part of filename\n",
    "    with open('MA of ' + str(df1.columns[i]) + '.csv', 'w') as csvfile:\n",
    "        mad.to_csv(csvfile) #writing the dataframe of observed and predicted values from AR Model to CSV file\n",
    "        \n",
    "    armamod(df1[keys[i]]) #Running ARMA Model for all the stocks\n",
    "    armam.append('%.3f'%sqrt(arma_mse)) #appending all RMSE values of the different stocks from ARMA Model to list 'armam'\n",
    "    #Create CSV file for predicted values of the corresponding model using the name of the corresponding stock as part of filename\n",
    "    with open('ARMA of ' + str(df1.columns[i]) + '.csv', 'w') as csvfile:\n",
    "        armad.to_csv(csvfile) #writing the dataframe of observed and predicted values from ARMA Model to CSV file\n",
    "    \n",
    "    arimamod(df1[keys[i]]) #Running the ARIMA Model for all the stocks\n",
    "    arimam.append('%.3f'%sqrt(arima_mse)) #appending all RMSE values of the different stocks from ARIMA Model to list 'arimam'\n",
    "    #Create CSV file for predicted values of the corresponding model using the name of the corresponding stock as part of filename\n",
    "    with open('ARIMA of ' + str(df1.columns[i]) + '.csv', 'w') as csvfile:\n",
    "        arimad.to_csv(csvfile) #writing the dataframe of observed and predicted values from ARIMA Model to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df1.columns)):\n",
    "    arimamod(df1[keys[i]]) #Running the ARIMA Model for all the stocks\n",
    "    arimam.append('%.3f'%sqrt(arima_mse)) #appending all RMSE values of the different stocks from ARIMA Model to list 'arimam'\n",
    "    #Create CSV file for predicted values of the corresponding model using the name of the corresponding stock as part of filename\n",
    "    with open('ARIMA of ' + str(df1.columns[i]) + '.csv', 'w') as csvfile:\n",
    "        arimad.to_csv(csvfile) #writing the dataframe of observed and predicted values from ARIMA Model to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "armse=[] #list to store RMSE values of all stocks obtained from AR Model in float format\n",
    "mamse=[] #list to store RMSE values of all stocks obtained from MA Model in float format\n",
    "armamse=[] #list to store RMSE values of all stocks obtained from ARMA Model in float format\n",
    "arimamse=[] #list to store RMSE values of all stocks obtained from ARIMA Model in float format\n",
    "\n",
    "for i in arm: #loop to iterate through all elements in list 'arm'\n",
    "    armse.append(float(i)) #converting string value to float and appending it to a new list 'armse'\n",
    "print(armse)    \n",
    "\n",
    "for i in mam: #loop to iterate through all elements in list 'mam'\n",
    "    mamse.append(float(i)) #converting string value to float and appending it to a new list 'mamse'\n",
    "print(mamse)   \n",
    "\n",
    "for i in armam: #loop to iterate through all elements in list 'armam'\n",
    "    armamse.append(float(i)) #converting string value to float and appending it to a new list 'armamse'\n",
    "print(armamse)\n",
    "\n",
    "for i in arimam: #loop to iterate through all elements in list 'arimam'\n",
    "    arimamse.append(float(i)) #converting string value to float and appending it to a new list 'arimamse'\n",
    "print(arimamse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMS Error Values of all Stocks by different models')\n",
    "#Creating array for comparing RMSE values of all stocks from all models\n",
    "re=np.column_stack((armse,mamse,armamse,arimamse))\n",
    "rt=pd.DataFrame(re) #Converting array to dataframe\n",
    "rt.index=df1.keys() #Assigning Index/Row values to name of corresponding Stock Name\n",
    "rt.columns=list(['AR','MA','ARMA','ARIMA']) #Assigning column names to the corresponding models\n",
    "rt #displaying the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = df1.pct_change() #percentage change matrix\n",
    "#calculate mean daily return and covariance of daily returns\n",
    "mean_daily_returns = returns.mean()\n",
    "cov_matrix = returns.cov()\n",
    " \n",
    "#set number of runs of random portfolio weights\n",
    "num_portfolios = len(df1)\n",
    " \n",
    "#set up array to hold results\n",
    "#We have increased the size of the array to hold the weight values for each stock\n",
    "results = np.zeros((4+len(df1.columns)-1, num_portfolios)) #array initialised to contain only zeros\n",
    "\n",
    "for i in range(num_portfolios):\n",
    "    weights = np.array(np.random.uniform(-0.3,0.3,12)) #assigning uniform weights to all stocks\n",
    "    \n",
    "    #calculate annualised portfolio return\n",
    "    portfolio_return = round(np.sum(mean_daily_returns * weights) * 252,2)\n",
    "    #calculate annualised portfolio volatility\n",
    "    portfolio_std_dev = round(np.sqrt(np.dot(weights.T,np.dot(cov_matrix, weights))) * np.sqrt(252),2)\n",
    " \n",
    "    #store results in results array\n",
    "    results[0,i] = portfolio_return\n",
    "    results[1,i] = portfolio_std_dev\n",
    "    \n",
    "    #store Sharpe Ratio (return / volatility) - risk free rate element excluded for simplicity\n",
    "    results[2,i] = results[0,i] / results[1,i]\n",
    "    \n",
    "    #iterate through the weight vector and add data to results array\n",
    "    for j in range(len(weights)):\n",
    "        results[j+3,i] = weights[j]\n",
    " \n",
    "#convert results array to Pandas DataFrame\n",
    "results_frame = pd.DataFrame(results.T,columns=['returns','volatality','sharpe',df1.columns[0],df1.columns[1],df1.columns[2],\n",
    "                                                df1.columns[3],df1.columns[4],df1.columns[5],df1.columns[6],df1.columns[7],\n",
    "                                                df1.columns[8],df1.columns[9],df1.columns[10],df1.columns[11]])\n",
    "results_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locate position of portfolio with highest Sharpe Ratio\n",
    "max_sharpe_port = results_frame.iloc[results_frame['sharpe'].idxmax()]\n",
    "#locate positon of portfolio with minimum standard deviation\n",
    "min_vol_port = results_frame.iloc[results_frame['volatality'].idxmin()]\n",
    " \n",
    "#create scatter plot coloured by Sharpe Ratio\n",
    "pyplot.scatter(results_frame.volatality,results_frame.returns,c=results_frame.sharpe,cmap='RdYlBu')\n",
    "pyplot.xlabel('Volatility')\n",
    "pyplot.ylabel('Returns')\n",
    "pyplot.colorbar()\n",
    "#plot red star to highlight position of portfolio with highest Sharpe Ratio\n",
    "pyplot.scatter(max_sharpe_port[1],max_sharpe_port[0],marker=(5,1,0),color='r',s=200)\n",
    "#plot green star to highlight position of minimum variance portfolio\n",
    "pyplot.scatter(min_vol_port[1],min_vol_port[0],marker=(5,1,0),color='g',s=200)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_sharpe_port)\n",
    "print(min_vol_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = df1.std()\n",
    "y = df1.mean()\n",
    "x = [i/max(x) for i in x]\n",
    "y= [(i-np.mean(y))/max(y) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shrat=[]\n",
    "for i,j in zip(x,y):\n",
    "    shrat.append(j/i)\n",
    "ind=shrat.index(max(shrat))\n",
    "print(shrat)\n",
    "print('Best Sharpe Ratio belongs to',df1.columns[ind] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.scatter(x, y)\n",
    "for index, symbol in enumerate(df1.columns):\n",
    "    pyplot.annotate(symbol, (x[index], y[index]))\n",
    "\n",
    "# Title and axis\n",
    "#pyplot.ylim([-1,1])\n",
    "pyplot.xlabel('Risk')\n",
    "pyplot.ylabel('Expected Return')\n",
    "pyplot.title('Expected Return versus Risk')\n",
    "# Save the plot\n",
    "pyplot.savefig('expected_return_vs_risk.png')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results_frame.index=df1.index\n",
    "results_frame.to_csv(\"Results Frame.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
